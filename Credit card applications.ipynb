{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a134e92a",
   "metadata": {},
   "source": [
    "# Aplicação para cartão de credito\n",
    "\n",
    "Grandes bancos comerciais hoje em dia recebem inumeras aplicações para cartão de credito as quais muitas são rejeitas e muitas são aprovadas por diversos motivos como renda, historico de credito entre outros. avaliar cada aplicação manualmente seria muitos custoso para os bancos e levaria bastante tempo. Com isso, nossa tarefa é criar um modelo de machine learning para avaliar cada futuro cliente, automatizando esse tarefa assim como os rais bancos fazer hoje em dia.\n",
    "\n",
    "<img src='201703100023430.PNG' style='width:1000px;height:500px'>\n",
    "\n",
    "O dataser utilizado foi do repositorio da UCI e esse notebook irá percorrer os seguintes passos:\n",
    "\n",
    "<li>Primeiro vamos carregar e vizualizar o conjunto de dados.</li>\n",
    "<li>Nós vamos perceber que exeistem diversos recursos que se misturam em númericos não númericos. em diferentes escalas e tambem existem dados ausente</li>\n",
    "<li>Nós faremos uma anilise exploratorio dos dados e logo em seguinda um pre_processamento para submeter esses dados a um modelo de machine learning.</li>\n",
    "<li>Por fim vamos construir um modelo de machine leaning para prever se determinado cliente terá seu cartão de credito aprovado ou rejeitas pelo banco.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32d52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando pacotes necessarios\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826b9a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando o dataset\n",
    "df = pd.read_csv('crx.data', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6226a",
   "metadata": {},
   "source": [
    "## 1. inspecionando o dataset\n",
    "\n",
    "de principo os dados parecem um pouco confuso pois todos eles foram anonimizados paara proteger a privacidade dos clientes, entretando nessa tabela temos dados que correspondem a <code>Genero</code>, <code>Idade</code>, <code>Dívida</code>, <code>Estado civil</code>, <code>cliente</code>, <code>Nivel Educacional</code>, <code>Etnia</code>, <code>Anos de Trabalho</code>, <code>Padrão Anterior</code>, <code>Empregado</code>, <code>Pontuação de Credito</code>, <code>Licença Automotiva</code>, <code>Cidadão</code>, <code>Codigo Postal</code>, <code>Renda</code> e finalmente <code>Status de Aprovação</code>. o que nós da um bom inicio para analizar nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e327c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Gender', 'Age', 'Debt', 'Married', 'BankCustomer', 'EducationLevel', 'Ethnicity', 'YearsEmployed', 'PriorDefault', 'Employed', 'CreditScore', 'DriversLicense', 'Citizen', 'ZipCode', 'Income', 'ApprovalStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a1b0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Gender          690 non-null    object \n",
      " 1   Age             690 non-null    object \n",
      " 2   Debt            690 non-null    float64\n",
      " 3   Married         690 non-null    object \n",
      " 4   BankCustomer    690 non-null    object \n",
      " 5   EducationLevel  690 non-null    object \n",
      " 6   Ethnicity       690 non-null    object \n",
      " 7   YearsEmployed   690 non-null    float64\n",
      " 8   PriorDefault    690 non-null    object \n",
      " 9   Employed        690 non-null    object \n",
      " 10  CreditScore     690 non-null    int64  \n",
      " 11  DriversLicense  690 non-null    object \n",
      " 12  Citizen         690 non-null    object \n",
      " 13  ZipCode         690 non-null    object \n",
      " 14  Income          690 non-null    int64  \n",
      " 15  ApprovalStatus  690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9460615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Debt</th>\n",
       "      <th>YearsEmployed</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>690.00000</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.758725</td>\n",
       "      <td>2.223406</td>\n",
       "      <td>2.40000</td>\n",
       "      <td>1017.385507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.978163</td>\n",
       "      <td>3.346513</td>\n",
       "      <td>4.86294</td>\n",
       "      <td>5210.102598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.207500</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>395.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>67.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Debt  YearsEmployed  CreditScore         Income\n",
       "count  690.000000     690.000000    690.00000     690.000000\n",
       "mean     4.758725       2.223406      2.40000    1017.385507\n",
       "std      4.978163       3.346513      4.86294    5210.102598\n",
       "min      0.000000       0.000000      0.00000       0.000000\n",
       "25%      1.000000       0.165000      0.00000       0.000000\n",
       "50%      2.750000       1.000000      0.00000       5.000000\n",
       "75%      7.207500       2.625000      3.00000     395.500000\n",
       "max     28.000000      28.500000     67.00000  100000.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c12db",
   "metadata": {},
   "source": [
    "## 2. Limpeza nos dados\n",
    "\n",
    "É possivel abservar que em alguns clientes há ausencia de informação e alguns dados estão com \"?\". com isso, é necessario imputar novos valores para esses registros ou retirar os registros que contenham \"?\". Veremos se há um numero significativo de registros com \"?\" no nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a625f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a informação ausente em NaN\n",
    "df = df.replace('?',np.NaN)\n",
    "df = df.replace('+',1)\n",
    "df = df.replace('-',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6cbb151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender            12\n",
       "Age               12\n",
       "Debt               0\n",
       "Married            6\n",
       "BankCustomer       6\n",
       "EducationLevel     9\n",
       "Ethnicity          9\n",
       "YearsEmployed      0\n",
       "PriorDefault       0\n",
       "Employed           0\n",
       "CreditScore        0\n",
       "DriversLicense     0\n",
       "Citizen            0\n",
       "ZipCode           13\n",
       "Income             0\n",
       "ApprovalStatus     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contando o numero de linhas\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deab6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covertendo Age para float\n",
    "df['Age'] = df['Age'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5a711",
   "metadata": {},
   "source": [
    "Como existe um pouco mais de 5% de informação ausente, optarei por apenas retiras essas observações do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8d0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retirando todas as linhas que continham informação ausente.\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01318320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender            0\n",
       "Age               0\n",
       "Debt              0\n",
       "Married           0\n",
       "BankCustomer      0\n",
       "EducationLevel    0\n",
       "Ethnicity         0\n",
       "YearsEmployed     0\n",
       "PriorDefault      0\n",
       "Employed          0\n",
       "CreditScore       0\n",
       "DriversLicense    0\n",
       "Citizen           0\n",
       "ZipCode           0\n",
       "Income            0\n",
       "ApprovalStatus    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39887f5f",
   "metadata": {},
   "source": [
    "## 3. Pre-processamento\n",
    "\n",
    "Agora que os valores ausentes foram devidamente tratados, nós ainda temos algumns trabalhos antes de submeter nossos dados a um modelo de machine learning.\n",
    "\n",
    "Temos duas tarefas principais, que são:\n",
    "\n",
    "   <li>Transformar as variaveis não-numericas e numericas.\n",
    "   <li>Deixar todos os dados na mesma escala.\n",
    "\n",
    "       \n",
    "\n",
    "Primeiro vamos transformar todos os dados em dados numericos, nao apenas pelo processamento ser mais rapido, como tambe alguns algoritmos de machine learning esperam receber dados numericos. faremos isso com a função get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e93cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidando com valores booleanos\n",
    "df.replace('t', 1, inplace = True)\n",
    "df.replace('f', 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f63149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar as variaveis não-numericas e numericas.\n",
    "df_gender = pd.get_dummies(df.Gender).add_prefix('gender_')\n",
    "df_married = pd.get_dummies(df.Married).add_prefix('Married_')\n",
    "df_bankCustomer = pd.get_dummies(df.BankCustomer).add_prefix('BankCustomer_')\n",
    "df_educationLevel = pd.get_dummies(df.EducationLevel).add_prefix('EducationLevel_')\n",
    "df_ethnicity = pd.get_dummies(df.Ethnicity).add_prefix('Ethnicity_')\n",
    "df_citizen = pd.get_dummies(df.Citizen).add_prefix('Citizen_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d54dabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluindo variaveis não-numericas.\n",
    "df.drop(columns = ['Gender','Married','BankCustomer','EducationLevel','Ethnicity','Citizen','ZipCode'], axis = 1, inplace = True)\n",
    "\n",
    "# Adicionando varieveis dummie.\n",
    "df_dummies = pd.concat([df, df_gender, df_married, df_bankCustomer, df_educationLevel, df_ethnicity, df_citizen], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ae9b4",
   "metadata": {},
   "source": [
    "## 3. Pre-processamento (II)\n",
    "\n",
    "Agora que temos todos os dados em valores numericos, vamos dividi-los em dados de treino e de teste para aplicar a normalização apenas na Features do nosso dataset e não na variavel target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a353a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo variaveis em features e target\n",
    "X = df_dummies.drop(['ApprovalStatus'], axis = 1)\n",
    "y = df_dummies['ApprovalStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c07dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a Normalização nas features\n",
    "norm = MinMaxScaler()\n",
    "X_norm = norm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fae4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em dados de Teste e Treino\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3a598",
   "metadata": {},
   "source": [
    "## 4. Criação de modelos\n",
    "\n",
    "Essencialmente, a previsão de aprovação de um cartão de credito é uma tarefa de classificação. de acordo com o dataset de UCI temos 690 observações das quais 55.5% foram rejeitadas e 44.5% foram aprovadas.\n",
    "\n",
    "Isso nos da um bom ponte de partida ja que nosso modelo terá que prever respeitando essa estatistica. Entretanto, qual modelo deveremos usar? nos proximos passos iremos criar alguns modelos de classificação e decidar qual o melhor deles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c94eeb",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um modelo padrão\n",
    "modelo_v1 = LogisticRegression()\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo_v1.fit(X_train,y_train)\n",
    "\n",
    "# Previsões com o modelo\n",
    "y_pred_v1 = modelo_v1.predict(X_test)\n",
    "\n",
    "# Previsões de probabilidade\n",
    "y_pred_proba_v1 = modelo_v1.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Roc_auc\n",
    "roc_auc_v1 = roc_auc_score(y_test,y_pred_proba_v1)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v1 = accuracy_score(y_test,y_pred_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdcb53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90, 20],\n",
       "       [10, 76]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de confusão\n",
    "confusion_matrix(y_test, y_pred_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cca3872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v1 = {'Nome': 'modelo_v1',\n",
    "                 'Algoritmo': 'Logisct Regression',\n",
    "                 'Roc_auc':roc_auc_v1,\n",
    "                 'Acuraci':acuracia_v1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a4284",
   "metadata": {},
   "source": [
    "### 4.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3afd5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo modelo.\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Definição dos parametros.\n",
    "param_dist_v2 = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"max_depth\": [8, 9, 10, 11],\n",
    "              \"min_samples_split\": [8, 10, 11, 14],\n",
    "              \"min_samples_leaf\": [1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "\n",
    "# Aplicando o RandomizedSearchCV.\n",
    "randomCV = RandomizedSearchCV(dtc, param_dist_v2, n_iter = 25,random_state=123)\n",
    "\n",
    "# Treinando os modelos\n",
    "randomCV.fit(X_train, y_train)\n",
    "\n",
    "# aplicando o melhor modelo\n",
    "modelo_v2 = randomCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce078f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo\n",
    "y_pred_v2 = modelo_v2.predict(X_test)\n",
    "\n",
    "# Previsões de probabilidade\n",
    "y_pred_proba_v2 = modelo_v2.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Roc_auc\n",
    "roc_auc_v2 = roc_auc_score(y_test,y_pred_proba_v2)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v2 = accuracy_score(y_test,y_pred_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dad7124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[96, 14],\n",
       "       [17, 69]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão.\n",
    "confusion_matrix(y_test, y_pred_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7691e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v2 = {'Nome': 'modelo_v2',\n",
    "                 'Algoritmo': 'Decision Tree',\n",
    "                 'Roc_auc':roc_auc_v2,\n",
    "                 'Acuraci':acuracia_v2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1234a64",
   "metadata": {},
   "source": [
    "### 4.3 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a65a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=7, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Definição dos parâmetros\n",
    "param_dist_v3 = {\"n_neighbors\": [1, 3, 5, 7],\n",
    "              \"weights\": ['uniform', 'distance']}\n",
    "\n",
    "# GridSearchCV\n",
    "gridcv_v3 = GridSearchCV(knn,\n",
    "                         param_dist_v3, \n",
    "                         scoring = 'roc_auc', \n",
    "                         n_jobs = -1)\n",
    "\n",
    "# treiamento do modelo\n",
    "gridcv_v3.fit(X_train,y_train)\n",
    "\n",
    "# Melhor modelo\n",
    "modelo_v3 = gridcv_v3.best_estimator_\n",
    "print(modelo_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50fd1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo\n",
    "y_pred_v3 = modelo_v3.predict(X_test)\n",
    "\n",
    "# Previsões de probabilidade\n",
    "y_pred_proba_v3 = modelo_v3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Roc_auc\n",
    "roc_auc_v3 = roc_auc_score(y_test,y_pred_proba_v3)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v3 = accuracy_score(y_test,y_pred_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ef63a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[89, 21],\n",
       "       [16, 70]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão.\n",
    "confusion_matrix(y_test, y_pred_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f5973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v3 = {'Nome': 'modelo_v3',\n",
    "                 'Algoritmo': 'KNN',\n",
    "                 'Roc_auc':roc_auc_v3,\n",
    "                 'Acuraci':acuracia_v3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb9642",
   "metadata": {},
   "source": [
    "### 4.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e3487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_features=8, min_samples_split=12, random_state=99)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = 99)\n",
    "\n",
    "# Definição dos parâmetros\n",
    "param_dist_v4 = {\"max_depth\": [1, 3, 7, None],\n",
    "              \"max_features\": [8, 9, 10, 11],\n",
    "              \"min_samples_split\": [8, 10, 12, 14],\n",
    "              \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Para o classificador criado com ExtraTrees, testamos diferentes combinações de parâmetros\n",
    "rsearch = RandomizedSearchCV(clf, \n",
    "                             param_distributions = param_dist_v4, \n",
    "                             n_iter = 25, \n",
    "                             return_train_score = True)  \n",
    "\n",
    "# Aplicando o resultado ao conjunto de dados de treino e obtendo o score\n",
    "rsearch.fit(X_train, y_train)\n",
    "\n",
    "# Resultados \n",
    "rsearch.cv_results_\n",
    "\n",
    "# Imprimindo o melhor estimador\n",
    "modelo_v4 = rsearch.best_estimator_\n",
    "print(modelo_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4954e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo\n",
    "y_pred_v4 = modelo_v4.predict(X_test)\n",
    "\n",
    "# Previsões de probabilidade\n",
    "y_pred_proba_v4 = modelo_v4.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Roc_auc\n",
    "roc_auc_v4 = roc_auc_score(y_test,y_pred_proba_v4)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v4 = accuracy_score(y_test,y_pred_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d1fbc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[95, 15],\n",
       "       [15, 71]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão.\n",
    "confusion_matrix(y_test, y_pred_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf26f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v4 = {'Nome': 'modelo_v4',\n",
    "                 'Algoritmo': 'Random Forest',\n",
    "                 'Roc_auc':roc_auc_v4,\n",
    "                 'Acuraci':acuracia_v4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eae48a",
   "metadata": {},
   "source": [
    "### 4.5 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "659c56cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo com Grid Search: 0.19962644577026367\n",
      "Acurácia em Treinamento: 88.40%\n",
      "\n",
      "Hiperparâmetros Ideais: {'coef0': 0.5, 'degree': 2, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# criação do modelo\n",
    "smv_v5 = SVC(kernel = 'poly')\n",
    "\n",
    "\n",
    "param_dist_v5 = {\"coef0\": np.array([0.5, 1]),\n",
    "              \"gamma\": np.array([0.001, 0.01]),\n",
    "              \"degree\": np.array([2, 3, 4])}\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "start = time.time()\n",
    "grid_search_poly = GridSearchCV(smv_v5, param_dist_v5, cv = 3)\n",
    "\n",
    "# Treinamento\n",
    "grid_search_poly.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print('Tempo de Treinamento do Modelo com Grid Search:', end - start)\n",
    "\n",
    "# Acurácia em Treino\n",
    "print(f\"Acurácia em Treinamento: {grid_search_poly.best_score_ :.2%}\")\n",
    "print(\"\")\n",
    "print(f\"Hiperparâmetros Ideais: {grid_search_poly.best_params_}\")\n",
    "\n",
    "modelo_v5 = grid_search_poly.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9722303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo\n",
    "y_pred_v5 = modelo_v5.predict(X_test)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v5 = accuracy_score(y_test,y_pred_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "447f62cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88, 22],\n",
       "       [ 6, 80]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão.\n",
    "confusion_matrix(y_test, y_pred_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a81d5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v5 = {'Nome': 'modelo_v5',\n",
    "                 'Algoritmo': 'SVM',\n",
    "                 'Roc_auc':np.NAN,\n",
    "                 'Acuraci':acuracia_v5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c352fc9",
   "metadata": {},
   "source": [
    "### 4.6 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01aac0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo com Grid Search: 69.69610691070557\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "param_dist_v6 = {\n",
    "    \"loss\":[\"log_loss\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "start = time.time()\n",
    "clf_gbc = GridSearchCV(gbc, param_dist_v6, cv=10, n_jobs=-1)\n",
    "\n",
    "clf_gbc.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print('Tempo de Treinamento do Modelo com Grid Search:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63d619ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparâmetros Ideais: {'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 0.1, 'min_samples_split': 0.31818181818181823, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hiperparâmetros Ideais: {clf_gbc.best_params_}\")\n",
    "\n",
    "modelo_v6 = clf_gbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c28032e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com o modelo\n",
    "y_pred_v6 = modelo_v6.predict(X_test)\n",
    "\n",
    "# Previsões de probabilidade\n",
    "y_pred_proba_v6 = modelo_v6.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Roc_auc\n",
    "roc_auc_v6 = roc_auc_score(y_test,y_pred_proba_v6)\n",
    "\n",
    "# acuracia\n",
    "acuracia_v6 = accuracy_score(y_test,y_pred_v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e3850fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97, 13],\n",
       "       [24, 62]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão.\n",
    "confusion_matrix(y_test, y_pred_v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28fa5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionario do modelo\n",
    "dict_modelo_v6 = {'Nome': 'modelo_v6',\n",
    "                 'Algoritmo': 'Gradient boosting',\n",
    "                 'Roc_auc':roc_auc_v6,\n",
    "                 'Acuraci':acuracia_v6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f71c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome</th>\n",
       "      <th>Algoritmo</th>\n",
       "      <th>Roc_auc</th>\n",
       "      <th>Acuraci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>modelo_v5</td>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelo_v1</td>\n",
       "      <td>Logisct Regression</td>\n",
       "      <td>0.899260</td>\n",
       "      <td>0.846939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modelo_v4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.929175</td>\n",
       "      <td>0.846939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modelo_v2</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.866543</td>\n",
       "      <td>0.841837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modelo_v3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.880708</td>\n",
       "      <td>0.811224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>modelo_v6</td>\n",
       "      <td>Gradient boosting</td>\n",
       "      <td>0.893552</td>\n",
       "      <td>0.811224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Nome           Algoritmo   Roc_auc   Acuraci\n",
       "4  modelo_v5                 SVM       NaN  0.857143\n",
       "0  modelo_v1  Logisct Regression  0.899260  0.846939\n",
       "3  modelo_v4       Random Forest  0.929175  0.846939\n",
       "1  modelo_v2       Decision Tree  0.866543  0.841837\n",
       "2  modelo_v3                 KNN  0.880708  0.811224\n",
       "5  modelo_v6   Gradient boosting  0.893552  0.811224"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modelos = pd.DataFrame([dict_modelo_v1,dict_modelo_v2,dict_modelo_v3,dict_modelo_v4,dict_modelo_v5,dict_modelo_v6])\n",
    "df_modelos.sort_values(by='Acuraci', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd16d1",
   "metadata": {},
   "source": [
    "## 5. Conclusão\n",
    "\n",
    "Terminamos esse notebook construindo um preditor de avaliação de cartao de credito usando 6 diferentes tipo de algoritmos de machine learning e tambem usando diferentes tecnicas de pre_processamento como normalização e variaveis dummies. usamos tambem GridSearchCV que realiza a tunação dos hiperparamentos dos algoritmos. conseguimos chegar a uma acuracia de 85% o que é bem plausivel para essa base de dados usando o algoritimo SVM o qual apresentou a melhor performace. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
